{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/Hugging_Face_transformers_topics/blob/main/indicTrans_python_interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjfzxXZLHed_",
        "outputId": "cdf4acaf-ee37-460d-aa0f-533f25edf4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 697, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 697 (delta 278), reused 344 (delta 240), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (697/697), 2.64 MiB | 23.09 MiB/s, done.\n",
            "Resolving deltas: 100% (405/405), done.\n",
            "/content/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1396, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 1396 (delta 133), reused 119 (delta 105), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1396/1396), 9.57 MiB | 14.49 MiB/s, done.\n",
            "Resolving deltas: 100% (743/743), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 16.33 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 19.40 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain"
      ],
      "metadata": {
        "id": "Zd394A0FhLcr",
        "outputId": "bb1b74c4-467c-4910-b15e-943856638d97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeYW2BJhlJvx",
        "outputId": "9dcc15a9-53aa-4b83-b583-da0a49ffb59b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting mock\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-1.2.2-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895239 sha256=295abe9c9ab02a7c330b9f68e62e5418a4a2a384ac833a39e3802560a6b70df3\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.92 mock-5.1.0 morfessor-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1 sacremoses-0.0.53 sphinx-argparse-0.4.0 sphinx-rtd-theme-1.2.2 sphinxcontrib-jquery-4.1 tensorboardX-2.6.2\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting docopt (from mosestokenizer)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile (from mosestokenizer)\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools (from mosestokenizer)\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (5.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (4.65.0)\n",
            "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49170 sha256=d3fadbbffdd5f655f339fe957131978f0b41b3bbea27dc83b1e9b00418d72acf\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d8/15/4c5ebbe883513f003cb055a0369c77c9df857023a706f39e70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=a792fb91e2b096d576357917dd9974e266ae35efc061c39059b44e4de076ad15\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3337 sha256=4aa0e86170c6e3e309070e2c268a5860418e7179df64ed6c9f810dda6c3f21ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6146 sha256=8b29c39694596d9f9b60ad2c901474ab691755f83a481b6c4b73e7c890686ecb\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ee/10/33257b0801ac6a912c841939032c16da1eb3db377afe1443e5\n",
            "Successfully built mosestokenizer docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, docopt, uctools, subword-nmt, mosestokenizer\n",
            "Successfully installed docopt-0.6.2 mosestokenizer-1.2.1 openfile-0.0.7 subword-nmt-0.3.8 toolwrapper-2.1.0 uctools-1.3.0\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 34769, done.\u001b[K\n",
            "remote: Total 34769 (delta 0), reused 0 (delta 0), pack-reused 34769\u001b[K\n",
            "Receiving objects: 100% (34769/34769), 25.00 MiB | 12.75 MiB/s, done.\n",
            "Resolving deltas: 100% (25221/25221), done.\n",
            "/content/fairseq\n",
            "Processing /content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (0.29.36)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.22.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2022.10.31)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.3.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.65.0)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.0.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.1)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.7.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->fairseq==0.12.2) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13->fairseq==0.12.2) (16.0.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=19924475 sha256=9128a97184caf2a5b33908b33443203bdf9cac6d2e502413a44096ac9b4d83cb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-57tmio4h/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=9c94929a6b27e66f76f1f682de4ebb8964c03ab8cbebdbad26094ed456e1980a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.1 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, xformers\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.29 typing-inspect-0.9.0 xformers-0.0.20\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "! pip install mosestokenizer subword-nmt\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TktUu9NW_PLq",
        "outputId": "4a48e8f9-5ce2-4087-f744-2dc0e1fac971"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n",
            "WARNING:root:Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n"
          ]
        }
      ],
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ],
      "metadata": {
        "id": "e7mopcRmx_aL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_4JxNdRlPQB",
        "outputId": "54a53dbb-1110-45d7-a2bc-cd487a337d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-07 13:12:43--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 164.52.210.96, 101.53.136.18, 101.53.152.33, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|164.52.210.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4811880516 (4.5G) [application/zip]\n",
            "Saving to: ‘en2indic.zip’\n",
            "\n",
            "en2indic.zip        100%[===================>]   4.48G  31.2MB/s    in 2m 15s  \n",
            "\n",
            "2023-08-07 13:14:59 (33.9 MB/s) - ‘en2indic.zip’ saved [4811880516/4811880516]\n",
            "\n",
            "Archive:  en2indic.zip\n",
            "   creating: en-indic/\n",
            "   creating: en-indic/vocab/\n",
            "  inflating: en-indic/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: en-indic/vocab/vocab.SRC  \n",
            "  inflating: en-indic/vocab/vocab.TGT  \n",
            "  inflating: en-indic/vocab/bpe_codes.32k.TGT  \n",
            "   creating: en-indic/final_bin/\n",
            "  inflating: en-indic/final_bin/preprocess.log  \n",
            "  inflating: en-indic/final_bin/dict.TGT.txt  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/dict.SRC.txt  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: en-indic/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: en-indic/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: en-indic/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: en-indic/model/\n",
            "  inflating: en-indic/model/checkpoint_best.pt  \n",
            "/content/indicTrans\n"
          ]
        }
      ],
      "source": [
        "# download the indictrans model\n",
        "\n",
        "\n",
        "# downloading the indic-en model\n",
        "#!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
        "#!unzip indic2en.zip\n",
        "\n",
        "# downloading the en-indic model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "!unzip en2indic.zip\n",
        "\n",
        "# # downloading the indic-indic model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip\n",
        "# !unzip m2m.zip\n",
        "\n",
        "%cd indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "u4QSzI7GUoQO",
        "outputId": "a4276124-6e6f-4aad-c381-87cb7c94da47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"model_files\")"
      ],
      "metadata": {
        "id": "FFqhvRWbUgFB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip en2indic.zip -d /content/model_files"
      ],
      "metadata": {
        "id": "_84obhmWUe5w",
        "outputId": "369ddb19-ea2c-4a84-911a-f745232830c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  en2indic.zip\n",
            "   creating: /content/model_files/en-indic/\n",
            "   creating: /content/model_files/en-indic/vocab/\n",
            "  inflating: /content/model_files/en-indic/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: /content/model_files/en-indic/vocab/vocab.SRC  \n",
            "  inflating: /content/model_files/en-indic/vocab/vocab.TGT  \n",
            "  inflating: /content/model_files/en-indic/vocab/bpe_codes.32k.TGT  \n",
            "   creating: /content/model_files/en-indic/final_bin/\n",
            "  inflating: /content/model_files/en-indic/final_bin/preprocess.log  \n",
            "  inflating: /content/model_files/en-indic/final_bin/dict.TGT.txt  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/dict.SRC.txt  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: /content/model_files/en-indic/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: /content/model_files/en-indic/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: /content/model_files/en-indic/model/\n",
            "  inflating: /content/model_files/en-indic/model/checkpoint_best.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indic to English Language Translation"
      ],
      "metadata": {
        "id": "RlCjOtGwEeKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTnWbHqY01-B",
        "outputId": "2104a004-864e-4af5-f2b2-94557f64110c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-16 06:42:54 | INFO | fairseq.tasks.translation | [SRC] dictionary: 35904 types\n",
            "2022-01-16 06:42:54 | INFO | fairseq.tasks.translation | [TGT] dictionary: 32088 types\n"
          ]
        }
      ],
      "source": [
        "from indicTrans.inference.engine import Model\n",
        "\n",
        "indic2en_model = Model(expdir='../indic-en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTp2NOgQ__sB",
        "outputId": "793a1356-700e-48fe-f497-129ab847f3a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 7273.36it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['He seems to know us.',\n",
              " 'We will follow you or continue to follow you',\n",
              " 'If you develop these symptoms in someone close to you, staying at home can help prevent the spread of Corona virus infection.']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ta_sents = ['அவனுக்கு நம்மைப் தெரியும் என்று தோன்றுகிறது',\n",
        "            \"நாங்கள் உன்னைத் பின்பற்றுவோம் (அ) தொடர்வோம். \",\n",
        "            'உங்களுக்கு உங்கள் அருகில் இருக்கும் ஒருவருக்கோ இத்தகைய அறிகுறிகள் தென்பட்டால், வீட்டிலேயே இருப்பது, கொரோனா வைரஸ் தொற்று பிறருக்கு வராமல் தடுக்க உதவும்.']\n",
        "\n",
        "\n",
        "indic2en_model.batch_translate(ta_sents, 'ta', 'en')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "VFXrCNZGEN7Z",
        "outputId": "143d20ae-92f5-4a8d-c9ed-d638a5bb3977"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 4304.06it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py:666: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = bbsz_idx // beam_size\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The pandemic has caused global social and economic disruption. This has led to the worlds largest recession since the Great Depression. This led to the postponement or cancellation of sporting, religious, political and cultural events. Due to this fear, there was a shortage of supply as most of the people purchased the items like masks, sanitizers etc.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "ta_paragraph = \"\"\"இத்தொற்றுநோய் உலகளாவிய சமூக மற்றும் பொருளாதார சீர்குலைவை ஏற்படுத்தியுள்ளது.இதனால் பெரும் பொருளாதார மந்தநிலைக்குப் பின்னர் உலகளவில் மிகப்பெரிய மந்தநிலை ஏற்பட்டுள்ளது. இது விளையாட்டு,மத, அரசியல் மற்றும் கலாச்சார நிகழ்வுகளை ஒத்திவைக்க அல்லது ரத்து செய்ய வழிவகுத்தது.\n",
        "அச்சம் காரணமாக முகக்கவசம், கிருமிநாசினி உள்ளிட்ட பொருட்களை அதிக நபர்கள் வாங்கியதால் விநியோகப் பற்றாக்குறை ஏற்பட்டது.\"\"\"\n",
        "\n",
        "indic2en_model.translate_paragraph(ta_paragraph, 'ta', 'en')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## English to Indian Language Translation"
      ],
      "metadata": {
        "id": "NHEf55bREnye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Hi_D7s_VIjis"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"Two Wheeler Loan Eligibility & List of Required Documents\n",
        "Applicable for the loan disbursed on/ after 1st April 2023\n",
        "Two wheeler loan allows middle class borrowers to pay monthly EMIs comfortably as most of them cannot afford to make the lump sum payment due to financial constraints.Once you decide to apply for a two wheeler loan, it is essential to check your two wheeler loan eligibility. Lenders are extremely cautious and sanction the loan only after checking the repayment capacity of the borrower. Thorough background checks are conducted before the loan is disbursed as there has been a sudden rise in number of defaulters over a period of time.\n",
        "To make sure your application for the loan is not declined; you must possess necessary documents and declare your exact source of income for speedy approval.\n",
        "Below are the general two wheeler loan eligibility criteria for two wheeler loans:\n",
        "\n",
        "Minimum age of the loan applicant should be at least 18 years at the time of applying for the loan, and the maximum age limit should be 65 years.\n",
        "Applicant should be residing in the same house for at least one year to show residential stability.\n",
        "Applicant should be in a stable job for at one year or self-employed with IT returns of two years.\n",
        "Applicant should have a good CIBIL score with credit bureaus\n",
        "Applicant should have a permanent telephone number and possess KYC and other related documents.\n",
        "What are factors affecting two-wheeler loan eligibility?\n",
        "\n",
        "Your two-wheeler loan eligibility depends on factors like your income, credit score, and current financial obligations.\n",
        "\n",
        "Credit score:One of the crucial factors considered by your lender to determine your eligibility is your credit score. It is a three-digit number that indicates whether or not you can repay the loan amount without any delays. You will need to maintain a minimum credit score of 750 to secure a two-wheeler loan.\n",
        "\n",
        "Income:Your two-wheeler loan eligibility also depends on your income. Since a two-wheeler loan is unsecured, you will have to convince your lender that you have a stable source of income. A higher-income will also ensure a lower interest rate.\n",
        "\n",
        "Outstanding loan:Having large outstanding debts will hurt your two-wheeler loan eligibility. If your lender finds that you have current obligations, you may not be offered a loan.\n",
        "\n",
        "Tips on how to improve your two-wheeler loan eligibility\n",
        "\n",
        "Two-wheeler loans can be improved using the following tips:\n",
        "\n",
        "Maintain a higher credit score: when you are applying for a two-wheeler loan, make sure that you have maintained a minimum credit score of 750.\n",
        "Choose a longer loan tenure: Longer loan tenure gives the borrower more time to pay off his EMI. Your two-wheeler loan eligibility increases when you choose a longer loan tenure.\n",
        "Opt for a joint loan: A co-applicant with a good credit score increases your eligibility for a two-wheeler loan.\n",
        "Show additional source of income: When you show your additional sources of income, it indicates that you can repay the loan amount without any delays or defaults.\n",
        "\n",
        "Documents Required for Two Wheeler Loan.Here is a list of documents required for a bike loan:\n",
        "\n",
        "a) Identity Proof:\n",
        "1. Aadhaar Card\n",
        "2. PAN card\n",
        "3. Passport\n",
        "4. Driving License\n",
        "5. Electoral Voter Identity Card\n",
        "\n",
        "b) Address Proof:\n",
        "1. Aadhaar Card\n",
        "2. PAN card\n",
        "3. Passport\n",
        "4. Driving License\n",
        "5. Latest Electricity Bill\n",
        "6. Latest Phone Bill\n",
        "7. Latest Bank Passbook reflecting the current address\n",
        "8. Property Documents\n",
        "9. Water bill\n",
        "\n",
        "c) Additional Documents for Salaried Persons\n",
        "\n",
        "c.1 )Salaried persons:\n",
        "1. Employment or offer letter\n",
        "2. Last salary slips <45 Days\n",
        "3. Last six months bank statements and latest form 16\n",
        "d) Self-Employed:\n",
        "1. Last three years income tax returns\n",
        "2. Last six months bank statements\n",
        "3. Sales tax returns\n",
        "4. TDS Certificate\n",
        "5. Company details\n",
        "\n",
        "Make sure to keep both originals and photocopies of the required documents for loan processing.Based on the above documents,the lender will process your two wheeler loan application, and you can expect a positive outcome on your loan approval.<<>>Hindi\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicTrans.inference.engine import Model\n",
        "\n",
        "indic2en_model = Model(expdir='../en-indic')"
      ],
      "metadata": {
        "id": "Aakdq-l3RLQ8",
        "outputId": "69b0add2-d777-4a6f-cbed-d12ea660a0d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "indic2en_model.translate_paragraph(context, 'en', 'or')"
      ],
      "metadata": {
        "id": "gis-MdTORp0i",
        "outputId": "5d1404fc-ed13-412d-f2c2-ed359c0c9173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 13.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.73 s, sys: 320 ms, total: 2.05 s\n",
            "Wall time: 5.92 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ଦୁଇ ଚକିଆ ଋଣ ଯୋଗ୍ୟତା ଏବଂ ଆବଶ୍ୟକୀୟ ଦସ୍ତାବିଜର ତାଲିକା ମଧ୍ୟବିତ୍ତ ବର୍ଗର ଋଣଗ୍ରହୀତାମାନଙ୍କୁ ମାସିକ ଇଏମଆଇ ପଇଠ କରିବାର ସୁବିଧା ପ୍ରଦାନ କରିଥାଏ, କାରଣ ସେମାନଙ୍କ ମଧ୍ୟରୁ ଅଧିକାଂଶ ଆର୍ଥିକ ପ୍ରତିବନ୍ଧକ କାରଣରୁ ଏକକାଳୀନ ପରିଶୋଧ କରିପାରିବେ ନାହିଁ। ଋଣଦାତାମାନେ ଅତ୍ୟନ୍ତ ସତର୍କତା ଅବଲମ୍ବନ କରିଥାନ୍ତି ଏବଂ ଋଣଗ୍ରହୀତାଙ୍କ ପରିଶୋଧ କ୍ଷମତାକୁ ଯାଞ୍ଚ କରିବା ପରେ ହିଁ ଋଣ ମଞ୍ଜୁର କରିଥାନ୍ତି। ଋଣ ପ୍ରଦାନ କରିବା ପୂର୍ବରୁ ବ୍ୟାକଗ୍ରାଉଣ୍ଡ ଯାଞ୍ଚ କରାଯାଏ କାରଣ କିଛି ସମୟ ମଧ୍ୟରେ ଋଣଖେଲାପୀଙ୍କ ସଂଖ୍ୟା ହଠାତ୍ ବୃଦ୍ଧି ପାଇଛି। ଋଣ ପାଇଁ ଆପଣଙ୍କର ଆବେଦନ ପ୍ରତ୍ୟାଖ୍ୟାନ ହୋଇନଥିବା ସୁନିଶ୍ଚିତ କରିବା। ତୁରନ୍ତ ଅନୁମୋଦନ ପାଇଁ ଆପଣଙ୍କ ପାଖରେ ଆବଶ୍ୟକ କାଗଜପତ୍ର ରହିବା ଆବଶ୍ୟକ ଏବଂ ଆୟର ସଠିକ ଉତ୍ସ ଘୋଷଣା କରିବା ଆବଶ୍ୟକ। ଦୁଇ ଚକିଆ ଯାନ ପାଇଁ ଋଣ ପାଇଁ ଯୋଗ୍ୟତା ସର୍ତ୍ତାବଳୀ ନିମ୍ନରେ ଦିଆଗଲାଃ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from fairseq import checkpoint_utils, options, tasks, utils\n",
        "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
        "from fairseq.token_generation_constraints import pack_constraints, unpack_constraints\n",
        "from fairseq_cli.generate import get_symbols_to_strip_from_output\n",
        "#\n",
        "from fairseq.models import register_model_architecture\n",
        "from fairseq.models.transformer import base_architecture\n",
        "#\n",
        "\n",
        "import codecs\n",
        "import os\n",
        "import json\n",
        "#\n",
        "from os import truncate\n",
        "from sacremoses import MosesPunctNormalizer\n",
        "from sacremoses import MosesTokenizer\n",
        "from sacremoses import MosesDetokenizer\n",
        "from subword_nmt.apply_bpe import BPE, read_vocabulary\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "from indicnlp.tokenize import indic_detokenize\n",
        "from indicnlp.normalize import indic_normalize\n",
        "from indicnlp.transliterate import unicode_transliterate\n",
        "from mosestokenizer import MosesSentenceSplitter\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "#\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils\n",
        "#\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "#Custom Interactive\n",
        "#\n",
        "Batch = namedtuple(\"Batch\", \"ids src_tokens src_lengths constraints\")\n",
        "Translation = namedtuple(\"Translation\", \"src_str hypos pos_scores alignments\")\n",
        "\n",
        "\n",
        "def make_batches(\n",
        "    lines, cfg, task, max_positions, encode_fn, constrainted_decoding=False\n",
        "):\n",
        "    def encode_fn_target(x):\n",
        "        return encode_fn(x)\n",
        "\n",
        "    if constrainted_decoding:\n",
        "        # Strip (tab-delimited) contraints, if present, from input lines,\n",
        "        # store them in batch_constraints\n",
        "        batch_constraints = [list() for _ in lines]\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"\\t\" in line:\n",
        "                lines[i], *batch_constraints[i] = line.split(\"\\t\")\n",
        "\n",
        "        # Convert each List[str] to List[Tensor]\n",
        "        for i, constraint_list in enumerate(batch_constraints):\n",
        "            batch_constraints[i] = [\n",
        "                task.target_dictionary.encode_line(\n",
        "                    encode_fn_target(constraint),\n",
        "                    append_eos=False,\n",
        "                    add_if_not_exist=False,\n",
        "                )\n",
        "                for constraint in constraint_list\n",
        "            ]\n",
        "\n",
        "    if constrainted_decoding:\n",
        "        constraints_tensor = pack_constraints(batch_constraints)\n",
        "    else:\n",
        "        constraints_tensor = None\n",
        "\n",
        "    tokens, lengths = task.get_interactive_tokens_and_lengths(lines, encode_fn)\n",
        "\n",
        "    itr = task.get_batch_iterator(\n",
        "        dataset=task.build_dataset_for_inference(\n",
        "            tokens, lengths, constraints=constraints_tensor\n",
        "        ),\n",
        "        max_tokens=cfg.dataset.max_tokens,\n",
        "        max_sentences=cfg.dataset.batch_size,\n",
        "        max_positions=max_positions,\n",
        "        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n",
        "    ).next_epoch_itr(shuffle=False)\n",
        "    for batch in itr:\n",
        "        ids = batch[\"id\"]\n",
        "        src_tokens = batch[\"net_input\"][\"src_tokens\"]\n",
        "        src_lengths = batch[\"net_input\"][\"src_lengths\"]\n",
        "        constraints = batch.get(\"constraints\", None)\n",
        "\n",
        "        yield Batch(\n",
        "            ids=ids,\n",
        "            src_tokens=src_tokens,\n",
        "            src_lengths=src_lengths,\n",
        "            constraints=constraints,\n",
        "        )\n",
        "\n",
        "\n",
        "class Translator:\n",
        "    def __init__(\n",
        "        self, model_path,data_dir, checkpoint_path, batch_size=25, constrained_decoding=False\n",
        "    ):\n",
        "\n",
        "        self.constrained_decoding = constrained_decoding\n",
        "        self.modl_path = model_path\n",
        "        self.parser = options.get_generation_parser(interactive=True)\n",
        "        # buffer_size is currently not used but we just initialize it to batch\n",
        "        # size + 1 to avoid any assertion errors.\n",
        "        if self.constrained_decoding:\n",
        "            self.parser.set_defaults(\n",
        "                path=checkpoint_path,\n",
        "                remove_bpe=\"subword_nmt\",\n",
        "                num_workers=-1,\n",
        "                constraints=\"ordered\",\n",
        "                batch_size=batch_size,\n",
        "                buffer_size=batch_size + 1,\n",
        "            )\n",
        "        else:\n",
        "            self.parser.set_defaults(\n",
        "                path=checkpoint_path,\n",
        "                remove_bpe=\"subword_nmt\",\n",
        "                num_workers=-1,\n",
        "                batch_size=batch_size,\n",
        "                buffer_size=batch_size + 1,\n",
        "            )\n",
        "        args = options.parse_args_and_arch(self.parser, input_args=[data_dir])\n",
        "        # we are explictly setting src_lang and tgt_lang here\n",
        "        # generally the data_dir we pass contains {split}-{src_lang}-{tgt_lang}.*.idx files from\n",
        "        # which fairseq infers the src and tgt langs(if these are not passed). In deployment we dont\n",
        "        # use any idx files and only store the SRC and TGT dictionaries.\n",
        "        args.source_lang = \"SRC\"\n",
        "        args.target_lang = \"TGT\"\n",
        "        # since we are truncating sentences to max_seq_len in engine, we can set it to False here\n",
        "        args.skip_invalid_size_inputs_valid_test = False\n",
        "\n",
        "        # we have custom architechtures in this folder and we will let fairseq\n",
        "        # import this\n",
        "        model_file_path = os.path.join(model_path,\"en-indic\",\"model_configs\")\n",
        "        args.user_dir = model_file_path\n",
        "        self.cfg = convert_namespace_to_omegaconf(args)\n",
        "\n",
        "        utils.import_user_module(self.cfg.common)\n",
        "\n",
        "        if self.cfg.interactive.buffer_size < 1:\n",
        "            self.cfg.interactive.buffer_size = 1\n",
        "        if self.cfg.dataset.max_tokens is None and self.cfg.dataset.batch_size is None:\n",
        "            self.cfg.dataset.batch_size = 1\n",
        "\n",
        "        assert (\n",
        "            not self.cfg.generation.sampling\n",
        "            or self.cfg.generation.nbest == self.cfg.generation.beam\n",
        "        ), \"--sampling requires --nbest to be equal to --beam\"\n",
        "        assert (\n",
        "            not self.cfg.dataset.batch_size\n",
        "            or self.cfg.dataset.batch_size <= self.cfg.interactive.buffer_size\n",
        "        ), \"--batch-size cannot be larger than --buffer-size\"\n",
        "\n",
        "        # Fix seed for stochastic decoding\n",
        "        # if self.cfg.common.seed is not None and not self.cfg.generation.no_seed_provided:\n",
        "        #     np.random.seed(self.cfg.common.seed)\n",
        "        #     utils.set_torch_seed(self.cfg.common.seed)\n",
        "\n",
        "        # if not self.constrained_decoding:\n",
        "        #     self.use_cuda = torch.cuda.is_available() and not self.cfg.common.cpu\n",
        "        # else:\n",
        "        #     self.use_cuda = False\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available() and not self.cfg.common.cpu\n",
        "\n",
        "        # Setup task, e.g., translation\n",
        "        self.task = tasks.setup_task(self.cfg.task)\n",
        "\n",
        "        # Load ensemble\n",
        "        overrides = ast.literal_eval(self.cfg.common_eval.model_overrides)\n",
        "        self.models, self._model_args = checkpoint_utils.load_model_ensemble(\n",
        "            utils.split_paths(self.cfg.common_eval.path),\n",
        "            arg_overrides=overrides,\n",
        "            task=self.task,\n",
        "            suffix=self.cfg.checkpoint.checkpoint_suffix,\n",
        "            strict=(self.cfg.checkpoint.checkpoint_shard_count == 1),\n",
        "            num_shards=self.cfg.checkpoint.checkpoint_shard_count,\n",
        "        )\n",
        "\n",
        "        # Set dictionaries\n",
        "        self.src_dict = self.task.source_dictionary\n",
        "        self.tgt_dict = self.task.target_dictionary\n",
        "\n",
        "        # Optimize ensemble for generation\n",
        "        for model in self.models:\n",
        "            if model is None:\n",
        "                continue\n",
        "            if self.cfg.common.fp16:\n",
        "                model.half()\n",
        "            if (\n",
        "                self.use_cuda\n",
        "                and not self.cfg.distributed_training.pipeline_model_parallel\n",
        "            ):\n",
        "                model.cuda()\n",
        "            model.prepare_for_inference_(self.cfg)\n",
        "\n",
        "        # Initialize generator\n",
        "        self.generator = self.task.build_generator(self.models, self.cfg.generation)\n",
        "\n",
        "        # Handle tokenization and BPE\n",
        "        self.tokenizer = self.task.build_tokenizer(self.cfg.tokenizer)\n",
        "        self.bpe = self.task.build_bpe(self.cfg.bpe)\n",
        "\n",
        "        # Load alignment dictionary for unknown word replacement\n",
        "        # (None if no unknown word replacement, empty if no path to align dictionary)\n",
        "        self.align_dict = utils.load_align_dict(self.cfg.generation.replace_unk)\n",
        "\n",
        "        self.max_positions = utils.resolve_max_positions(\n",
        "            self.task.max_positions(), *[model.max_positions() for model in self.models]\n",
        "        )\n",
        "\n",
        "    def encode_fn(self, x):\n",
        "        if self.tokenizer is not None:\n",
        "            x = self.tokenizer.encode(x)\n",
        "        if self.bpe is not None:\n",
        "            x = self.bpe.encode(x)\n",
        "        return x\n",
        "\n",
        "    def decode_fn(self, x):\n",
        "        if self.bpe is not None:\n",
        "            x = self.bpe.decode(x)\n",
        "        if self.tokenizer is not None:\n",
        "            x = self.tokenizer.decode(x)\n",
        "        return x\n",
        "\n",
        "    def translate(self, inputs, constraints=None):\n",
        "        if self.constrained_decoding and constraints is None:\n",
        "            raise ValueError(\"Constraints cant be None in constrained decoding mode\")\n",
        "        if not self.constrained_decoding and constraints is not None:\n",
        "            raise ValueError(\"Cannot pass constraints during normal translation\")\n",
        "        if constraints:\n",
        "            constrained_decoding = True\n",
        "            modified_inputs = []\n",
        "            for _input, constraint in zip(inputs, constraints):\n",
        "                modified_inputs.append(_input + f\"\\t{constraint}\")\n",
        "            inputs = modified_inputs\n",
        "        else:\n",
        "            constrained_decoding = False\n",
        "\n",
        "        start_id = 0\n",
        "        results = []\n",
        "        final_translations = []\n",
        "        for batch in make_batches(\n",
        "            inputs,\n",
        "            self.cfg,\n",
        "            self.task,\n",
        "            self.max_positions,\n",
        "            self.encode_fn,\n",
        "            constrained_decoding,\n",
        "        ):\n",
        "            bsz = batch.src_tokens.size(0)\n",
        "            src_tokens = batch.src_tokens\n",
        "            src_lengths = batch.src_lengths\n",
        "            constraints = batch.constraints\n",
        "            if self.use_cuda:\n",
        "                src_tokens = src_tokens.cuda()\n",
        "                src_lengths = src_lengths.cuda()\n",
        "                if constraints is not None:\n",
        "                    constraints = constraints.cuda()\n",
        "\n",
        "            sample = {\n",
        "                \"net_input\": {\n",
        "                    \"src_tokens\": src_tokens,\n",
        "                    \"src_lengths\": src_lengths,\n",
        "                },\n",
        "            }\n",
        "\n",
        "            translations = self.task.inference_step(\n",
        "                self.generator, self.models, sample, constraints=constraints\n",
        "            )\n",
        "\n",
        "            list_constraints = [[] for _ in range(bsz)]\n",
        "            if constrained_decoding:\n",
        "                list_constraints = [unpack_constraints(c) for c in constraints]\n",
        "            for i, (id, hypos) in enumerate(zip(batch.ids.tolist(), translations)):\n",
        "                src_tokens_i = utils.strip_pad(src_tokens[i], self.tgt_dict.pad())\n",
        "                constraints = list_constraints[i]\n",
        "                results.append(\n",
        "                    (\n",
        "                        start_id + id,\n",
        "                        src_tokens_i,\n",
        "                        hypos,\n",
        "                        {\n",
        "                            \"constraints\": constraints,\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        # sort output to match input order\n",
        "        for id_, src_tokens, hypos, _ in sorted(results, key=lambda x: x[0]):\n",
        "            src_str = \"\"\n",
        "            if self.src_dict is not None:\n",
        "                src_str = self.src_dict.string(\n",
        "                    src_tokens, self.cfg.common_eval.post_process\n",
        "                )\n",
        "\n",
        "            # Process top predictions\n",
        "            for hypo in hypos[: min(len(hypos), self.cfg.generation.nbest)]:\n",
        "                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
        "                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n",
        "                    src_str=src_str,\n",
        "                    alignment=hypo[\"alignment\"],\n",
        "                    align_dict=self.align_dict,\n",
        "                    tgt_dict=self.tgt_dict,\n",
        "                    remove_bpe=\"subword_nmt\",\n",
        "                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n",
        "                        self.generator\n",
        "                    ),\n",
        "                )\n",
        "                detok_hypo_str = self.decode_fn(hypo_str)\n",
        "                final_translations.append(detok_hypo_str)\n",
        "        return final_translations\n",
        "    #\n",
        "    #engine\n",
        "    INDIC = [\"as\", \"bn\", \"gu\", \"hi\", \"kn\", \"ml\", \"mr\", \"or\", \"pa\", \"ta\", \"te\"]\n",
        "\n",
        "\n",
        "def split_sentences(paragraph, language):\n",
        "    INDIC = [\"as\", \"bn\", \"gu\", \"hi\", \"kn\", \"ml\", \"mr\", \"or\", \"pa\", \"ta\", \"te\"]\n",
        "    if language == \"en\":\n",
        "        with MosesSentenceSplitter(language) as splitter:\n",
        "            return splitter([paragraph])\n",
        "    elif language in INDIC:\n",
        "        return sentence_tokenize.sentence_split(paragraph, lang=language)\n",
        "\n",
        "\n",
        "def add_token(sent, tag_infos):\n",
        "    \"\"\"add special tokens specified by tag_infos to each element in list\n",
        "    tag_infos: list of tuples (tag_type,tag)\n",
        "    each tag_info results in a token of the form: __{tag_type}__{tag}__\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = []\n",
        "    for tag_type, tag in tag_infos:\n",
        "        token = \"__\" + tag_type + \"__\" + tag + \"__\"\n",
        "        tokens.append(token)\n",
        "\n",
        "    return \" \".join(tokens) + \" \" + sent\n",
        "\n",
        "\n",
        "def apply_lang_tags(sents, src_lang, tgt_lang):\n",
        "    tagged_sents = []\n",
        "    for sent in sents:\n",
        "        tagged_sent = add_token(sent.strip(), [(\"src\", src_lang), (\"tgt\", tgt_lang)])\n",
        "        tagged_sents.append(tagged_sent)\n",
        "    return tagged_sents\n",
        "\n",
        "\n",
        "def truncate_long_sentences(sents):\n",
        "\n",
        "    MAX_SEQ_LEN = 1000\n",
        "    new_sents = []\n",
        "\n",
        "    for sent in sents:\n",
        "        words = sent.split()\n",
        "        num_words = len(words)\n",
        "        if num_words > MAX_SEQ_LEN:\n",
        "            print_str = \" \".join(words[:5]) + \" .... \" + \" \".join(words[-5:])\n",
        "            sent = \" \".join(words[:MAX_SEQ_LEN])\n",
        "            print(\n",
        "                f\"WARNING: Sentence {print_str} truncated to 1000 tokens as it exceeds maximum length limit\"\n",
        "            )\n",
        "\n",
        "        new_sents.append(sent)\n",
        "    return new_sents\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, expdir,model_path):\n",
        "        self.expdir = expdir\n",
        "        self.model_path = model_path\n",
        "        self.en_tok = MosesTokenizer(lang=\"en\")\n",
        "        self.en_normalizer = MosesPunctNormalizer()\n",
        "        self.en_detok = MosesDetokenizer(lang=\"en\")\n",
        "        self.xliterator = unicode_transliterate.UnicodeIndicTransliterator()\n",
        "        print(\"Initializing vocab and bpe\")\n",
        "        self.vocabulary = read_vocabulary(\n",
        "            codecs.open(f\"{expdir}/vocab/vocab.SRC\", encoding=\"utf-8\"), 5\n",
        "        )\n",
        "        self.bpe = BPE(\n",
        "            codecs.open(f\"{expdir}/vocab/bpe_codes.32k.SRC\", encoding=\"utf-8\"),\n",
        "            -1,\n",
        "            \"@@\",\n",
        "            self.vocabulary,\n",
        "            None,\n",
        "        )\n",
        "\n",
        "        print(\"Initializing model for translation\")\n",
        "        # initialize the model\n",
        "        self.translator = Translator(\n",
        "            model_path,f\"{expdir}/final_bin\", f\"{expdir}/model/checkpoint_best.pt\", batch_size=100\n",
        "        )\n",
        "\n",
        "    # translate a batch of sentences from src_lang to tgt_lang\n",
        "    def batch_translate(self, batch, src_lang, tgt_lang):\n",
        "\n",
        "        assert isinstance(batch, list)\n",
        "        preprocessed_sents = self.preprocess(batch, lang=src_lang)\n",
        "        bpe_sents = self.apply_bpe(preprocessed_sents)\n",
        "        tagged_sents = apply_lang_tags(bpe_sents, src_lang, tgt_lang)\n",
        "        tagged_sents = truncate_long_sentences(tagged_sents)\n",
        "\n",
        "        translations = self.translator.translate(tagged_sents)\n",
        "        postprocessed_sents = self.postprocess(translations, tgt_lang)\n",
        "\n",
        "        return postprocessed_sents\n",
        "\n",
        "    # translate a paragraph from src_lang to tgt_lang\n",
        "    def translate_paragraph(self, paragraph, src_lang, tgt_lang):\n",
        "\n",
        "        assert isinstance(paragraph, str)\n",
        "        sents = split_sentences(paragraph, src_lang)\n",
        "\n",
        "        postprocessed_sents = self.batch_translate(sents, src_lang, tgt_lang)\n",
        "\n",
        "        translated_paragraph = \" \".join(postprocessed_sents)\n",
        "\n",
        "        return translated_paragraph\n",
        "\n",
        "    def preprocess_sent(self, sent, normalizer, lang):\n",
        "        if lang == \"en\":\n",
        "            return \" \".join(\n",
        "                self.en_tok.tokenize(\n",
        "                    self.en_normalizer.normalize(sent.strip()), escape=False\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            # line = indic_detokenize.trivial_detokenize(line.strip(), lang)\n",
        "            return unicode_transliterate.UnicodeIndicTransliterator.transliterate(\n",
        "                \" \".join(\n",
        "                    indic_tokenize.trivial_tokenize(\n",
        "                        normalizer.normalize(sent.strip()), lang\n",
        "                    )\n",
        "                ),\n",
        "                lang,\n",
        "                \"hi\",\n",
        "            ).replace(\" ् \", \"्\")\n",
        "\n",
        "    def preprocess(self, sents, lang):\n",
        "        \"\"\"\n",
        "        Normalize, tokenize and script convert(for Indic)\n",
        "        return number of sentences input file\n",
        "        \"\"\"\n",
        "\n",
        "        if lang == \"en\":\n",
        "\n",
        "            # processed_sents = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
        "            #     delayed(preprocess_line)(line, None, lang) for line in tqdm(sents, total=num_lines)\n",
        "            # )\n",
        "            processed_sents = [\n",
        "                self.preprocess_sent(line, None, lang) for line in tqdm(sents)\n",
        "            ]\n",
        "\n",
        "        else:\n",
        "            normfactory = indic_normalize.IndicNormalizerFactory()\n",
        "            normalizer = normfactory.get_normalizer(lang)\n",
        "\n",
        "            # processed_sents = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
        "            #     delayed(preprocess_line)(line, normalizer, lang) for line in tqdm(infile, total=num_lines)\n",
        "            # )\n",
        "            processed_sents = [\n",
        "                self.preprocess_sent(line, normalizer, lang) for line in tqdm(sents)\n",
        "            ]\n",
        "\n",
        "        return processed_sents\n",
        "\n",
        "    def postprocess(self, sents, lang, common_lang=\"hi\"):\n",
        "        \"\"\"\n",
        "        parse fairseq interactive output, convert script back to native Indic script (in case of Indic languages) and detokenize.\n",
        "        infname: fairseq log file\n",
        "        outfname: output file of translation (sentences not translated contain the dummy string 'DUMMY_OUTPUT'\n",
        "        input_size: expected number of output sentences\n",
        "        lang: language\n",
        "        \"\"\"\n",
        "        postprocessed_sents = []\n",
        "\n",
        "        if lang == \"en\":\n",
        "            for sent in sents:\n",
        "                # outfile.write(en_detok.detokenize(sent.split(\" \")) + \"\\n\")\n",
        "                postprocessed_sents.append(self.en_detok.detokenize(sent.split(\" \")))\n",
        "        else:\n",
        "            for sent in sents:\n",
        "                outstr = indic_detokenize.trivial_detokenize(\n",
        "                    self.xliterator.transliterate(sent, common_lang, lang), lang\n",
        "                )\n",
        "                # outfile.write(outstr + \"\\n\")\n",
        "                postprocessed_sents.append(outstr)\n",
        "        return postprocessed_sents\n",
        "\n",
        "    def apply_bpe(self, sents):\n",
        "\n",
        "        return [self.bpe.process_line(sent) for sent in sents]\n",
        "#\n",
        "#\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "                            chunk_size=500,\n",
        "                            chunk_overlap=0,\n",
        "                            separator = \"\\n\",\n",
        "                            length_function = len,\n",
        "                            )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "# inference script execution template\n",
        "#\n",
        "def preprocess_function(request_data, content_type=None):\n",
        "    #with open(text_path,\"r\",encoding='utf-8') as f:\n",
        "    #    data = f.read()\n",
        "    #print(data)\n",
        "    # pass the textand the target tanguage to be translated separated by a \";\" semicolon\n",
        "    data = request_data['data']\n",
        "    data_list = data.split(\"<<>>\")\n",
        "    text = data_list[0]\n",
        "    target = data_list[1].strip()\n",
        "    print(f\"target:{target}\")\n",
        "    return (text,target)\n",
        "\n",
        "#\n",
        "#\n",
        "def predict_function(context, model):\n",
        "    text,target = context\n",
        "    #\n",
        "    print(target)\n",
        "    INDIC = {\"Assamese\": \"as\", \"Bengali\": \"bn\", \"Gujarati\": \"gu\", \"Hindi\": \"hi\",\"Kannada\": \"kn\",\"Malayalam\": \"ml\", \"Marathi\": \"mr\", \"Odia\": \"or\",\"Punjabi\": \"pa\",\"Tamil\": \"ta\", \"Telugu\" : \"te\"}\n",
        "    #\n",
        "    record_texts = get_text_chunks(text)\n",
        "    txt = \" \"\n",
        "    for texts in record_texts:\n",
        "      input_batch = split_sentences(texts, language=\"en\")\n",
        "      print(input_batch)\n",
        "      answer = model.batch_translate(input_batch, 'en', INDIC[target])\n",
        "      #answer = model.translate_paragraph(input_batch, 'en', INDIC[target])\n",
        "      txt += \" \".join(answer)\n",
        "    return txt\n",
        "\n",
        "#\n",
        "def model_load_function(model_path):\n",
        "    model_file_path = os.path.join(model_path, \"en-indic\")\n",
        "    indic2en_model = Model(model_path = model_path,expdir=model_file_path)\n",
        "\n",
        "    return indic2en_model\n",
        "\n",
        "\n",
        "#\n",
        "def postprocess_function(predictions, content_type=None):\n",
        "    return json.dumps({\"response\":predictions},ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "## Test the script\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    txt_path = \"/content/input.txt\"\n",
        "    data = preprocess_function(txt_path)\n",
        "    model_path = \"/content/drive/MyDrive/indic/model_files\"\n",
        "    path = model_load_function(model_path)\n",
        "    predictions = predict_function(data,path)\n",
        "    out = postprocess_function(predictions)\n",
        "    print(out)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "KVXm54qlRkEB",
        "outputId": "5b266314-6c03-476c-fb0f-36aef5f880e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \\'__main__\\':\\n    txt_path = \"/content/input.txt\"\\n    data = preprocess_function(txt_path)\\n    model_path = \"/content/drive/MyDrive/indic/model_files\"\\n    path = model_load_function(model_path)\\n    predictions = predict_function(data,path)\\n    out = postprocess_function(predictions)\\n    print(out)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "content = context\n",
        "request_data = {\"data\":content}\n",
        "data = preprocess_function(request_data)"
      ],
      "metadata": {
        "id": "vfTcY1SKTWcL",
        "outputId": "a675e009-998d-4a17-ddec-887e186d062c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target:Hindi\n",
            "CPU times: user 396 µs, sys: 98 µs, total: 494 µs\n",
            "Wall time: 443 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"files\")"
      ],
      "metadata": {
        "id": "rVwTKWK2Ycva"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/mydrive\")"
      ],
      "metadata": {
        "id": "jM3fsKxWVsS8",
        "outputId": "d45bde54-f6ff-4acb-dede-e9ffb51525ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/mydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model_path = \"/content/files\"\n",
        "path = model_load_function(model_path)"
      ],
      "metadata": {
        "id": "XsPrQV9EVWKV",
        "outputId": "84459f6e-ccc7-45b8-a7dd-248ba7d238e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n",
            "CPU times: user 10.4 s, sys: 5.72 s, total: 16.1 s\n",
            "Wall time: 48 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "predictions = predict_function(data,path)"
      ],
      "metadata": {
        "id": "OGrao0DMcZJk",
        "outputId": "b9e44f82-87d1-40b2-d887-35b8fb3a8781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 537, which is longer than the specified 500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi\n",
            "['Two Wheeler Loan Eligibility & List of Required Documents Applicable for the loan disbursed on/ after 1st April 2023']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1791.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Two wheeler loan allows middle class borrowers to pay monthly EMIs comfortably as most of them cannot afford to make the lump sum payment due to financial constraints.Once you decide to apply for a two wheeler loan, it is essential to check your two wheeler loan eligibility.', 'Lenders are extremely cautious and sanction the loan only after checking the repayment capacity of the borrower.', 'Thorough background checks are conducted before the loan is disbursed as there has been a sudden rise in number of defaulters over a period of time.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 1159.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To make sure your application for the loan is not declined;', 'you must possess necessary documents and declare your exact source of income for speedy approval.', 'Below are the general two wheeler loan eligibility criteria for two wheeler loans:']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 2409.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Applicant should be in a stable job for at one year or self-employed with IT returns of two years.', 'Applicant should have a good CIBIL score with credit bureaus Applicant should have a permanent telephone number and possess KYC and other related documents.', 'What are factors affecting two-wheeler loan eligibility?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 1199.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Credit score:', 'One of the crucial factors considered by your lender to determine your eligibility is your credit score.', 'It is a three-digit number that indicates whether or not you can repay the loan amount without any delays.', 'You will need to maintain a minimum credit score of 750 to secure a two-wheeler loan.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 1793.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Income:', 'Your two-wheeler loan eligibility also depends on your income.', 'Since a two-wheeler loan is unsecured, you will have to convince your lender that you have a stable source of income.', 'A higher-income will also ensure a lower interest rate.', 'Outstanding loan:', 'Having large outstanding debts will hurt your two-wheeler loan eligibility.', 'If your lender finds that you have current obligations, you may not be offered a loan.', 'Tips on how to improve your two-wheeler loan eligibility']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 3167.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Two-wheeler loans can be improved using the following tips:', 'Maintain a higher credit score:', 'when you are applying for a two-wheeler loan, make sure that you have maintained a minimum credit score of 750.', 'Choose a longer loan tenure:', 'Longer loan tenure gives the borrower more time to pay off his EMI.', 'Your two-wheeler loan eligibility increases when you choose a longer loan tenure.', 'Opt for a joint loan:', 'A co-applicant with a good credit score increases your eligibility for a two-wheeler loan.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 2388.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Show additional source of income:', 'When you show your additional sources of income, it indicates that you can repay the loan amount without any delays or defaults.', 'Documents Required for Two Wheeler Loan.Here is a list of documents required for a bike loan:', 'a) Identity Proof:', '1.', 'Aadhaar Card 2.', 'PAN card 3.', 'Passport 4.', 'Driving License 5.', 'Electoral Voter Identity Card b) Address Proof:', '1.', 'Aadhaar Card 2.', 'PAN card 3.', 'Passport 4.', 'Driving License 5.', 'Latest Electricity Bill 6.', 'Latest Phone Bill']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:00<00:00, 4983.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['7.', 'Latest Bank Passbook reflecting the current address 8.', 'Property Documents 9.', 'Water bill c) Additional Documents for Salaried Persons c.1 )Salaried persons:', '1.', 'Employment or offer letter 2.', 'Last salary slips <45 Days 3.', 'Last six months bank statements and latest form 16 d) Self-Employed:', '1.', 'Last three years income tax returns 2.', 'Last six months bank statements 3.', 'Sales tax returns 4.', 'TDS Certificate 5.', 'Company details']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 3356.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Make sure to keep both originals and photocopies of the required documents for loan processing.Based on the above documents,the lender will process your two wheeler loan application, and you can expect a positive outcome on your loan approval.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 628.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.3 s, sys: 10.1 ms, total: 4.31 s\n",
            "Wall time: 4.53 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "out = postprocess_function(predictions)\n",
        "out"
      ],
      "metadata": {
        "id": "IS4hbb8cf_Gq",
        "outputId": "4fb7aca5-5bdb-4fb0-a209-dbb50a48363f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 105 µs, sys: 0 ns, total: 105 µs\n",
            "Wall time: 109 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n    \"response\": \" दुपहिया ऋण पात्रता और 1 अप्रैल 2023 को या उसके बाद वितरित ऋण के लिए लागू आवश्यक दस्तावेजों की सूची दुपहिया ऋण मध्यम वर्ग के उधारकर्ताओं को मासिक ईएमआई का आराम से भुगतान करने की अनुमति देता है क्योंकि उनमें से अधिकांश वित्तीय बाधाओं के कारण एकमुश्त भुगतान नहीं कर सकते हैं ऋणदाता बेहद सतर्क रहते हैं और ऋणकर्ता की पुनर्भुगतान क्षमता की जांच करने के बाद ही ऋण को मंजूरी देते हैं। ऋण संवितरित करने से पहले पृष्ठभूमि की गहन जांच की जाती है क्योंकि पिछले कुछ समय में डिफॉल्टरों की संख्या में अचानक वृद्धि हुई है। यह सुनिश्चित करने के लिए कि ऋण के लिए आपका आवेदन अस्वीकृत न किया गया हो आपके पास आवश्यक दस्तावेज होने चाहिए और शीघ्र अनुमोदन के लिए आय के सही स्रोत की घोषणा करनी चाहिए। दुपहिया ऋणों के लिए सामान्य दुपहिया ऋण पात्रता मानदंड निम्नलिखित हैंःक्रेडिट स्कोरः आपकी पात्रता निर्धारित करने के लिए आपके ऋणदाता द्वारा विचार किए जाने वाले महत्वपूर्ण कारकों में से एक आपका क्रेडिट स्कोर है। यह एक तीन अंकों का नंबर होता है जो बताता है कि आप बिना किसी देरी के ऋण राशि का भुगतान कर सकते हैं या नहीं। दुपहिया वाहन ऋण प्राप्त करने के लिए आपको कम से कम 750 का क्रेडिट स्कोर बनाए रखना होगा।आय: आपकी दुपहिया वाहन ऋण पात्रता आपकी आय पर भी निर्भर करती है। चूंकि दोपहिया वाहन ऋण असुरक्षित है, इसलिए आपको अपने ऋणदाता को यह समझाना होगा कि आपके पास आय का एक स्थिर स्रोत है। ज्यादा आय से भी कम ब्याज मिलेगा।बकाया ऋणः बड़े बकाया ऋण होने से आपकी दुपहिया वाहन ऋण पात्रता प्रभावित होगी। यदि आपके ऋणदाता को पता चलता है कि आपके पास वर्तमान दायित्व हैं, तो आपको ऋण की पेशकश नहीं की जा सकती है।उच्च क्रेडिट स्कोर बनाए रखेंः जब आप दोपहिया वाहन ऋण के लिए आवेदन कर रहे हैं, तो सुनिश्चित करें कि आपका न्यूनतम क्रेडिट स्कोर 750 है। लंबी अवधि के ऋण का चयन करेंः ऋण की लंबी अवधि उधारकर्ता को अपनी ईएमआई का भुगतान करने के लिए अधिक समय देती है। आपकी दुपहिया वाहन ऋण पात्रता तब बढ़ जाती है जब आप एक लंबी ऋण अवधि चुनते हैं। संयुक्त ऋण का विकल्पः अच्छे क्रेडिट स्कोर के साथ एक सह-आवेदक दोपहिया ऋण के लिए आपकी पात्रता को बढ़ाता है। आय का अतिरिक्त स्रोत दिखाएंः जब आप अपनी आय के अतिरिक्त स्रोत दिखाते हैं, तो यह इंगित करता है कि आप बिना किसी देरी या चूक के ऋण की राशि का भुगतान कर सकते हैं।दुपहिया वाहन ऋण के लिए आवश्यक दस्तावेजख) एड्रेस प्रूफः 1 है। आधार कार्ड 2. पैन कार्ड 3. पासपोर्ट 4. ड्राइविंग लाइसेंस 5. नवीनतम बिजली बिल 6. नवीनतम फोन बिल 7. नवीनतम बैंक पासबुक जो वर्तमान पते को दर्शाती है 8. संपत्ति दस्तावेज 9. पानी का बिल सी) वेतनभोगी व्यक्तियों के लिए अतिरिक्त दस्तावेज सी) वेतनभोगी व्यक्ति: 1 है। रोजगार या प्रस्ताव पत्र 2. अंतिम वेतन पर्चियां <45 दिन 3. पिछले छह महीने के बैंक स्टेटमेंट और नवीनतम फॉर्म 16 डी) स्व-नियोजित: 1 है। पिछले तीन वर्षों का आयकर रिटर्न 2. पिछले छह महीने के बैंक स्टेटमेंट 3. बिक्री कर रिटर्न 4. टीडीएस प्रमाण पत्र 5. उपर्युक्त दस्तावेजों के आधार पर, ऋणदाता आपके दुपहिया ऋण आवेदन को प्रोसेस करेगा और आप अपने ऋण अनुमोदन पर सकारात्मक परिणाम की उम्मीद कर सकते हैं।\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "    \"response\": \" ଦୁଇ ଚକିଆ ଋଣ ଯୋଗ୍ୟତା ଏବଂ ଆବଶ୍ୟକୀୟ ଦସ୍ତାବିଜର ତାଲିକା ମଧ୍ୟବିତ୍ତ ବର୍ଗର ଋଣଗ୍ରହୀତାମାନଙ୍କୁ ମାସିକ ଇଏମଆଇ ପଇଠ କରିବାର ସୁବିଧା ପ୍ରଦାନ କରିଥାଏ, କାରଣ ସେମାନଙ୍କ ମଧ୍ୟରୁ ଅଧିକାଂଶ ଆର୍ଥିକ ପ୍ରତିବନ୍ଧକ କାରଣରୁ ଏକକାଳୀନ ପରିଶୋଧ କରିପାରିବେ ନାହିଁ। ଋଣଦାତାମାନେ ଅତ୍ୟନ୍ତ ସତର୍କତା ଅବଲମ୍ବନ କରିଥାନ୍ତି ଏବଂ ଋଣଗ୍ରହୀତାଙ୍କ ପରିଶୋଧ କ୍ଷମତାକୁ ଯାଞ୍ଚ କରିବା ପରେ ହିଁ ଋଣ ମଞ୍ଜୁର କରିଥାନ୍ତି। ଋଣ ପ୍ରଦାନ କରିବା ପୂର୍ବରୁ ବ୍ୟାକଗ୍ରାଉଣ୍ଡ ଯାଞ୍ଚ କରାଯାଏ କାରଣ କିଛି ସମୟ ମଧ୍ୟରେ ଋଣଖେଲାପୀଙ୍କ ସଂଖ୍ୟା ହଠାତ୍ ବୃଦ୍ଧି ପାଇଛି। ଋଣ ପାଇଁ ଆପଣଙ୍କର ଆବେଦନ ପ୍ରତ୍ୟାଖ୍ୟାନ ହୋଇନଥିବା ସୁନିଶ୍ଚିତ କରିବା। ତୁରନ୍ତ ଅନୁମୋଦନ ପାଇଁ ଆପଣଙ୍କ ପାଖରେ ଆବଶ୍ୟକ କାଗଜପତ୍ର ରହିବା ଆବଶ୍ୟକ ଏବଂ ଆୟର ସଠିକ ଉତ୍ସ ଘୋଷଣା କରିବା ଆବଶ୍ୟକ। ଦୁଇ ଚକିଆ ଯାନ ପାଇଁ ଋଣ ପାଇଁ ଯୋଗ୍ୟତା ସର୍ତ୍ତାବଳୀ ନିମ୍ନରେ ଦିଆଗଲାଃକ୍ରେଡିଟ୍ ସ୍କୋର: ଆପଣଙ୍କ ଯୋଗ୍ୟତା ନିର୍ଦ୍ଧାରଣ କରିବା ପାଇଁ ଋଣଦାତାଙ୍କ ଦ୍ୱାରା ବିବେଚନା କରାଯାଉଥିବା ଗୁରୁତ୍ୱପୂର୍ଣ୍ଣ ବିଷୟଗୁଡ଼ିକ ମଧ୍ୟରୁ ଗୋଟିଏ ହେଉଛି ଆପଣଙ୍କ କ୍ରେଡିଟ ସ୍କୋର। ଏହା ହେଉଛି ଏକ ତିନି ଅଙ୍କ ବିଶିଷ୍ଟ ନମ୍ବର ଯାହା ସୂଚାଇଥାଏ କି ଆପଣ ବିନା ବିଳମ୍ବରେ ଋଣ ପରିଶୋଧ କରିପାରିବେ କି ନାହିଁ। ଦୁଇ ଚକିଆ ଯାନ ପାଇଁ ଋଣ ନେବା ପାଇଁ ଆପଣଙ୍କୁ ସର୍ବନିମ୍ନ 750 କ୍ରେଡିଟ ସ୍କୋର ରଖିବାକୁ ପଡ଼ିବ।ଆୟ: ଆପଣଙ୍କ ଦୁଇ ଚକିଆ ଯାନ ଋଣ ଯୋଗ୍ୟତା ମଧ୍ୟ ଆପଣଙ୍କ ଆୟ ଉପରେ ନିର୍ଭର କରିଥାଏ। ଯେହେତୁ ଦୁଇ ଚକିଆ ଯାନ ପାଇଁ ଋଣ ଅସୁରକ୍ଷିତ, ସେଥିପାଇଁ ଆପଣଙ୍କୁ ଋଣଦାତାଙ୍କୁ ବୁଝାଇବାକୁ ପଡ଼ିବ ଯେ ଆପଣଙ୍କର ଆୟର ଏକ ସ୍ଥିର ଉତ୍ସ ରହିଛି। ଅଧିକ ଆୟ ହେଲେ ସୁଧ ହାର ମଧ୍ୟ କମ୍ ରହିବ।ବକେୟା ଋଣଃ ବଡ଼ ବକେୟା ଋଣ ଆପଣଙ୍କ ଦୁଇ ଚକିଆ ଯାନ ଋଣ ଯୋଗ୍ୟତା ଉପରେ କୁପ୍ରଭାବ ପକାଇବ। ଯଦି ଆପଣଙ୍କ ଋଣଦାତା ଜାଣନ୍ତି ଯେ ଆପଣଙ୍କର ବର୍ତ୍ତମାନର ଦାୟିତ୍ୱ ରହିଛି, ତେବେ ଆପଣଙ୍କୁ ଋଣ ଦିଆଯିବ ନାହିଁ।ଦୁଇ ଚକିଆ ଯାନ ପାଇଁ ଋଣ ଯୋଗ୍ୟତାକୁ କିପରି ସୁଧାରିବେ ଜାଣନ୍ତୁଅଧିକ କ୍ରେଡିଟ ସ୍କୋର ବଜାୟ ରଖନ୍ତୁଃ ଯେତେବେଳେ ଆପଣ ଦୁଇ ଚକିଆ ଯାନ ପାଇଁ ଋଣ ପାଇଁ ଆବେଦନ କରୁଛନ୍ତି, ଧ୍ୟାନ ରଖନ୍ତୁ ଯେ ଆପଣଙ୍କର ସର୍ବନିମ୍ନ କ୍ରେଡିଟ ସ୍କୋର 750 ରହିଥିବ। ଦୀର୍ଘକାଳୀନ ଋଣ ଅବଧି ବାଛନ୍ତୁ: ଅଧିକ ସମୟ ପର୍ଯ୍ୟନ୍ତ ଋଣ ନେବା ଦ୍ୱାରା ଋଣଗ୍ରହୀତାଙ୍କୁ ଇଏମଆଇ ପଇଠ କରିବା ପାଇଁ ଅଧିକ ସମୟ ମିଳିଥାଏ। ଯେତେବେଳେ ଆପଣ ଅଧିକ ସମୟ ପର୍ଯ୍ୟନ୍ତ ଋଣ ନେବା ପାଇଁ ନିଷ୍ପତ୍ତି ନିଅନ୍ତି, ସେତେବେଳେ ଆପଣଙ୍କର ଦୁଇ ଚକିଆ ଯାନ ଋଣ ଯୋଗ୍ୟତା ବୃଦ୍ଧି ପାଇଥାଏ। ଯୁଗ୍ମ ଋଣ ପାଇଁ ବିକଳ୍ପ: ଭଲ କ୍ରେଡିଟ୍ ସ୍କୋର ଥିବା ସହ ଆବେଦନକାରୀ ଦୁଇ ଚକିଆ ଯାନ ପାଇଁ ଋଣ ପାଇଁ ଯୋଗ୍ୟତା ବଢ଼ାଇଥାଏ। ଆୟର ଅତିରିକ୍ତ ଉତ୍ସ ଦେଖାନ୍ତୁ: ଯେତେବେଳେ ଆପଣ ନିଜର ଅତିରିକ୍ତ ଆୟର ଉତ୍ସ ଦେଖାଇବେ, ଏହା ସୂଚାଏ ଯେ ଆପଣ ବିନା କୌଣସି ବିଳମ୍ବ କିମ୍ବା ଡିଫଲ୍ଟ ବିନା ଋଣ ରାଶି ପରିଶୋଧ କରିପାରିବେ।ଦୁଇ ଚକିଆ ଯାନ ପାଇଁ ଋଣ ପାଇଁ ଆବଶ୍ୟକ ଦସ୍ତାବିଜ ନିମ୍ନରେ ପ୍ରଦାନ କରାଗଲାଃପରିଚୟ ପ୍ରମାଣଃ ଆଧାର କାର୍ଡ ପାନ୍ କାର୍ଡ ଡ୍ରାଇଭିଂ ଲାଇସେନ୍ସ ଭୋଟର ପରିଚୟ ପତ୍ର ଠିକଣା ପ୍ରମାଣ ଆଧାର କାର୍ଡ ପାନ୍ କାର୍ଡ ଡ୍ରାଇଭିଂ ଲାଇସେନ୍ସ ଡ୍ରାଇଭିଂ ଲାଇସେନ୍ସ ନବୀନତମ ବିଦ୍ୟୁତ ବିଲ ନବୀନତମ ଫୋନ୍ ବିଲ ନବୀନତମ ବ୍ୟାଙ୍କ ପାସବୁକ ଯାହା ସମ୍ପତି ବିଲ୍ ଏବଂ ଜଳ ବିଲ୍ ଉପରେ ପ୍ରତିଫଳିତ ହେଉଛିଦରମା ପାଉଥିବା ବ୍ୟକ୍ତିଙ୍କ ପାଇଁ ଅତିରିକ୍ତ କାଗଜପତ୍ରଗତ ତିନି ବର୍ଷର ଆୟକର ରିଟର୍ଣ୍ଣ ଗତ 6 ମାସର ବ୍ୟାଙ୍କ ବିବରଣୀ ବିକ୍ରି ଟିଡିଏସ ସାର୍ଟିଫିକେଟ କମ୍ପାନୀ ବିବରଣୀଋଣ ପ୍ରକ୍ରିୟାକରଣ ପାଇଁ ଉଭୟ ମୌଳିକ ଏବଂ ଫୋଟୋକପି ରଖିବା ସୁନିଶ୍ଚିତ କରନ୍ତୁ। ଉପରୋକ୍ତ ଦସ୍ତାବିଜ ଆଧାରରେ ଋଣଦାତା ଆପଣଙ୍କ ଦୁଇ ଚକିଆ ଋଣ ଆବେଦନକୁ ପ୍ରକ୍ରିୟାକରଣ କରିବେ ଏବଂ ଆପଣଙ୍କର ଋଣ ଅନୁମୋଦନ ଉପରେ ସକାରାତ୍ମକ ଫଳାଫଳ ଆଶା କରିପାରିବେ।\"\n",
        "}"
      ],
      "metadata": {
        "id": "RBT50myhkWya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "out = postprocess_function(predictions)\n",
        "out"
      ],
      "metadata": {
        "id": "_Y86LejhkYU7",
        "outputId": "e813267a-529f-423c-d80a-87f8401191c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 89 µs, sys: 19 µs, total: 108 µs\n",
            "Wall time: 113 µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n    \"response\": \" दुपहिया ऋण पात्रता और 1 अप्रैल 2023 को या उसके बाद वितरित ऋण के लिए लागू आवश्यक दस्तावेजों की सूचीदुपहिया ऋण मध्यम वर्ग के उधारकर्ताओं को मासिक ईएमआई का भुगतान आराम से करने की अनुमति देता है क्योंकि उनमें से अधिकांश वित्तीय बाधाओं के कारण एकमुश्त भुगतान नहीं कर सकते हैं। एक बार जब आप दुपहिया ऋण के लिए आवेदन करने का फैसला करते हैं, तो आपके दुपहिया ऋण की पात्रता की जांच करना आवश्यक है। ऋणदाता बेहद सतर्क रहते हैं और ऋणकर्ता की पुनर्भुगतान क्षमता की जांच करने के बाद ही ऋण को मंजूरी देते हैं। ऋण संवितरित करने से पहले पृष्ठभूमि की गहन जांच की जाती है क्योंकि पिछले कुछ समय में डिफॉल्टरों की संख्या में अचानक वृद्धि हुई है।यह सुनिश्चित करने के लिए कि ऋण के लिए आपका आवेदन अस्वीकृत न किया गया हो आपके पास आवश्यक दस्तावेज होने चाहिए और शीघ्र अनुमोदन के लिए आय के सही स्रोत की घोषणा करनी चाहिए। दुपहिया ऋणों के लिए सामान्य दुपहिया ऋण पात्रता मानदंड निम्नलिखित हैंःआवेदक को एक वर्ष के लिए स्थायी नौकरी या दो वर्ष के आईटी रिटर्न के साथ स्व-नियोजित होना चाहिए। आवेदक के पास क्रेडिट ब्यूरो के साथ एक अच्छा सिबिल स्कोर होना चाहिए आवेदक के पास एक स्थायी टेलीफोन नंबर होना चाहिए और केवाईसी और अन्य संबंधित दस्तावेज होने चाहिए। दुपहिया वाहन ऋण पात्रता को प्रभावित करने वाले कारक क्या हैं?क्रेडिट स्कोरः आपकी पात्रता निर्धारित करने के लिए आपके ऋणदाता द्वारा विचार किए जाने वाले महत्वपूर्ण कारकों में से एक आपका क्रेडिट स्कोर है। यह एक तीन अंकों का नंबर होता है जो बताता है कि आप बिना किसी देरी के ऋण राशि का भुगतान कर सकते हैं या नहीं। दुपहिया वाहन ऋण प्राप्त करने के लिए आपको कम से कम 750 का क्रेडिट स्कोर बनाए रखना होगा।आय: आपकी दुपहिया वाहन ऋण पात्रता आपकी आय पर भी निर्भर करती है। चूंकि दोपहिया वाहन ऋण असुरक्षित है, इसलिए आपको अपने ऋणदाता को यह समझाना होगा कि आपके पास आय का एक स्थिर स्रोत है। ज्यादा आय से भी कम ब्याज मिलेगा। बकाया ऋणः बड़े बकाया ऋण होने से आपकी दुपहिया वाहन ऋण पात्रता प्रभावित होगी। यदि आपके ऋणदाता को पता चलता है कि आपके पास वर्तमान दायित्व हैं, तो आपको ऋण की पेशकश नहीं की जा सकती है। दुपहिया वाहन ऋण की पात्रता में सुधार के लिए सुझावदुपहिया ऋणों में निम्नलिखित सुझावों का उपयोग करके सुधार किया जा सकता हैः उच्च क्रेडिट स्कोर बनाए रखेंः जब आप दोपहिया वाहन ऋण के लिए आवेदन कर रहे हैं, तो सुनिश्चित करें कि आपका न्यूनतम क्रेडिट स्कोर 750 है। लंबी अवधि के ऋण का चयन करेंः ऋण की लंबी अवधि उधारकर्ता को अपनी ईएमआई का भुगतान करने के लिए अधिक समय देती है। आपकी दुपहिया वाहन ऋण पात्रता तब बढ़ जाती है जब आप एक लंबी ऋण अवधि चुनते हैं। संयुक्त ऋण का विकल्पः अच्छे क्रेडिट स्कोर के साथ एक सह-आवेदक दोपहिया ऋण के लिए आपकी पात्रता को बढ़ाता है।आय का अतिरिक्त स्रोत दिखाएंः जब आप अपनी आय के अतिरिक्त स्रोत दिखाते हैं, तो यह इंगित करता है कि आप बिना किसी देरी या चूक के ऋण की राशि का भुगतान कर सकते हैं। दुपहिया वाहन ऋण के लिए आवश्यक दस्तावेज क) पहचान प्रमाण: 1 है। आधार कार्ड 2. पैन कार्ड 3. पासपोर्ट 4. ड्राइविंग लाइसेंस 5. मतदाता पहचान पत्र (बी) पते का प्रमाण: 1 है। आधार कार्ड 2. पैन कार्ड 3. पासपोर्ट 4. ड्राइविंग लाइसेंस 5. नवीनतम बिजली बिल 6. नवीनतम फोन बिल7. नवीनतम बैंक पासबुक जो वर्तमान पते को दर्शाती है 8. संपत्ति दस्तावेज 9. पानी का बिल सी) वेतनभोगी व्यक्तियों के लिए अतिरिक्त दस्तावेज सी) वेतनभोगी व्यक्ति: 1 है। रोजगार या प्रस्ताव पत्र 2. अंतिम वेतन पर्चियां <45 दिन 3. पिछले छह महीने के बैंक स्टेटमेंट और नवीनतम फॉर्म 16 डी) स्व-नियोजित: 1 है। पिछले तीन वर्षों का आयकर रिटर्न 2. पिछले छह महीने के बैंक स्टेटमेंट 3. बिक्री कर रिटर्न 4. टीडीएस प्रमाण पत्र 5. कंपनी का विवरणउपर्युक्त दस्तावेजों के आधार पर, ऋणदाता आपके दुपहिया ऋण आवेदन को प्रोसेस करेगा और आप अपने ऋण अनुमोदन पर सकारात्मक परिणाम की उम्मीद कर सकते हैं।\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Translate"
      ],
      "metadata": {
        "id": "ypKLXtu4r5zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "    \"response\": \" दुपहिया ऋण पात्रता और 1 अप्रैल 2023 को या उसके बाद वितरित ऋण के लिए लागू आवश्यक दस्तावेजों की सूचीदुपहिया ऋण मध्यम वर्ग के उधारकर्ताओं को मासिक ईएमआई का भुगतान आराम से करने की अनुमति देता है क्योंकि उनमें से अधिकांश वित्तीय बाधाओं के कारण एकमुश्त भुगतान नहीं कर सकते हैं। एक बार जब आप दुपहिया ऋण के लिए आवेदन करने का फैसला करते हैं, तो आपके दुपहिया ऋण की पात्रता की जांच करना आवश्यक है। ऋणदाता बेहद सतर्क रहते हैं और ऋणकर्ता की पुनर्भुगतान क्षमता की जांच करने के बाद ही ऋण को मंजूरी देते हैं। ऋण संवितरित करने से पहले पृष्ठभूमि की गहन जांच की जाती है क्योंकि पिछले कुछ समय में डिफॉल्टरों की संख्या में अचानक वृद्धि हुई है।यह सुनिश्चित करने के लिए कि ऋण के लिए आपका आवेदन अस्वीकृत न किया गया हो आपके पास आवश्यक दस्तावेज होने चाहिए और शीघ्र अनुमोदन के लिए आय के सही स्रोत की घोषणा करनी चाहिए। दुपहिया ऋणों के लिए सामान्य दुपहिया ऋण पात्रता मानदंड निम्नलिखित हैंःआवेदक को एक वर्ष के लिए स्थायी नौकरी या दो वर्ष के आईटी रिटर्न के साथ स्व-नियोजित होना चाहिए। आवेदक के पास क्रेडिट ब्यूरो के साथ एक अच्छा सिबिल स्कोर होना चाहिए आवेदक के पास एक स्थायी टेलीफोन नंबर होना चाहिए और केवाईसी और अन्य संबंधित दस्तावेज होने चाहिए। दुपहिया वाहन ऋण पात्रता को प्रभावित करने वाले कारक क्या हैं?क्रेडिट स्कोरः आपकी पात्रता निर्धारित करने के लिए आपके ऋणदाता द्वारा विचार किए जाने वाले महत्वपूर्ण कारकों में से एक आपका क्रेडिट स्कोर है। यह एक तीन अंकों का नंबर होता है जो बताता है कि आप बिना किसी देरी के ऋण राशि का भुगतान कर सकते हैं या नहीं। दुपहिया वाहन ऋण प्राप्त करने के लिए आपको कम से कम 750 का क्रेडिट स्कोर बनाए रखना होगा।आय: आपकी दुपहिया वाहन ऋण पात्रता आपकी आय पर भी निर्भर करती है। चूंकि दोपहिया वाहन ऋण असुरक्षित है, इसलिए आपको अपने ऋणदाता को यह समझाना होगा कि आपके पास आय का एक स्थिर स्रोत है। ज्यादा आय से भी कम ब्याज मिलेगा। बकाया ऋणः बड़े बकाया ऋण होने से आपकी दुपहिया वाहन ऋण पात्रता प्रभावित होगी। यदि आपके ऋणदाता को पता चलता है कि आपके पास वर्तमान दायित्व हैं, तो आपको ऋण की पेशकश नहीं की जा सकती है। दुपहिया वाहन ऋण की पात्रता में सुधार के लिए सुझावदुपहिया ऋणों में निम्नलिखित सुझावों का उपयोग करके सुधार किया जा सकता हैः उच्च क्रेडिट स्कोर बनाए रखेंः जब आप दोपहिया वाहन ऋण के लिए आवेदन कर रहे हैं, तो सुनिश्चित करें कि आपका न्यूनतम क्रेडिट स्कोर 750 है। लंबी अवधि के ऋण का चयन करेंः ऋण की लंबी अवधि उधारकर्ता को अपनी ईएमआई का भुगतान करने के लिए अधिक समय देती है। आपकी दुपहिया वाहन ऋण पात्रता तब बढ़ जाती है जब आप एक लंबी ऋण अवधि चुनते हैं। संयुक्त ऋण का विकल्पः अच्छे क्रेडिट स्कोर के साथ एक सह-आवेदक दोपहिया ऋण के लिए आपकी पात्रता को बढ़ाता है।आय का अतिरिक्त स्रोत दिखाएंः जब आप अपनी आय के अतिरिक्त स्रोत दिखाते हैं, तो यह इंगित करता है कि आप बिना किसी देरी या चूक के ऋण की राशि का भुगतान कर सकते हैं। दुपहिया वाहन ऋण के लिए आवश्यक दस्तावेज क) पहचान प्रमाण: 1 है। आधार कार्ड 2. पैन कार्ड 3. पासपोर्ट 4. ड्राइविंग लाइसेंस 5. मतदाता पहचान पत्र (बी) पते का प्रमाण: 1 है। आधार कार्ड 2. पैन कार्ड 3. पासपोर्ट 4. ड्राइविंग लाइसेंस 5. नवीनतम बिजली बिल 6. नवीनतम फोन बिल7. नवीनतम बैंक पासबुक जो वर्तमान पते को दर्शाती है 8. संपत्ति दस्तावेज 9. पानी का बिल सी) वेतनभोगी व्यक्तियों के लिए अतिरिक्त दस्तावेज सी) वेतनभोगी व्यक्ति: 1 है। रोजगार या प्रस्ताव पत्र 2. अंतिम वेतन पर्चियां <45 दिन 3. पिछले छह महीने के बैंक स्टेटमेंट और नवीनतम फॉर्म 16 डी) स्व-नियोजित: 1 है। पिछले तीन वर्षों का आयकर रिटर्न 2. पिछले छह महीने के बैंक स्टेटमेंट 3. बिक्री कर रिटर्न 4. टीडीएस प्रमाण पत्र 5. कंपनी का विवरणउपर्युक्त दस्तावेजों के आधार पर, ऋणदाता आपके दुपहिया ऋण आवेदन को प्रोसेस करेगा और आप अपने ऋण अनुमोदन पर सकारात्मक परिणाम की उम्मीद कर सकते हैं।\"\n",
        "}"
      ],
      "metadata": {
        "id": "3GUUEd2DsA9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Translate Paragraph"
      ],
      "metadata": {
        "id": "jOye_bmApbgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "    \"response\": \" दुपहिया ऋण पात्रता और 1 अप्रैल 2023 को या उसके बाद वितरित ऋण के लिए लागू आवश्यक दस्तावेजों की सूची दुपहिया ऋण मध्यम वर्ग के उधारकर्ताओं को मासिक ईएमआई का आराम से भुगतान करने की अनुमति देता है क्योंकि उनमें से अधिकांश वित्तीय बाधाओं के कारण एकमुश्त भुगतान नहीं कर सकते हैं ऋणदाता बेहद सतर्क रहते हैं और ऋणकर्ता की पुनर्भुगतान क्षमता की जांच करने के बाद ही ऋण को मंजूरी देते हैं। ऋण संवितरित करने से पहले पृष्ठभूमि की गहन जांच की जाती है क्योंकि पिछले कुछ समय में डिफॉल्टरों की संख्या में अचानक वृद्धि हुई है। यह सुनिश्चित करने के लिए कि ऋण के लिए आपका आवेदन अस्वीकृत न किया गया हो आपके पास आवश्यक दस्तावेज होने चाहिए और शीघ्र अनुमोदन के लिए आय के सही स्रोत की घोषणा करनी चाहिए। दुपहिया ऋणों के लिए सामान्य दुपहिया ऋण पात्रता मानदंड निम्नलिखित हैंःक्रेडिट स्कोरः आपकी पात्रता निर्धारित करने के लिए आपके ऋणदाता द्वारा विचार किए जाने वाले महत्वपूर्ण कारकों में से एक आपका क्रेडिट स्कोर है। यह एक तीन अंकों का नंबर होता है जो बताता है कि आप बिना किसी देरी के ऋण राशि का भुगतान कर सकते हैं या नहीं। दुपहिया वाहन ऋण प्राप्त करने के लिए आपको कम से कम 750 का क्रेडिट स्कोर बनाए रखना होगा।उच्च क्रेडिट स्कोर बनाए रखेंः जब आप दोपहिया वाहन ऋण के लिए आवेदन कर रहे हैं, तो सुनिश्चित करें कि आपका न्यूनतम क्रेडिट स्कोर 750 है। लंबी अवधि के ऋण का चयन करेंः ऋण की लंबी अवधि उधारकर्ता को अपनी ईएमआई का भुगतान करने के लिए अधिक समय देती है। आपकी दुपहिया वाहन ऋण पात्रता तब बढ़ जाती है जब आप एक लंबी ऋण अवधि चुनते हैं। संयुक्त ऋण का विकल्पः अच्छे क्रेडिट स्कोर के साथ एक सह-आवेदक दोपहिया ऋण के लिए आपकी पात्रता को बढ़ाता है। आय का अतिरिक्त स्रोत दिखाएंः जब आप अपनी आय के अतिरिक्त स्रोत दिखाते हैं, तो यह इंगित करता है कि आप बिना किसी देरी या चूक के ऋण की राशि का भुगतान कर सकते हैं।ख) एड्रेस प्रूफः 1 है। आधार कार्ड 2. पैन कार्ड 3. पासपोर्ट 4. ड्राइविंग लाइसेंस 5. नवीनतम बिजली बिल 6. नवीनतम फोन बिल 7. नवीनतम बैंक पासबुक जो वर्तमान पते को दर्शाती है 8. संपत्ति दस्तावेज 9. पानी का बिल सी) वेतनभोगी व्यक्तियों के लिए अतिरिक्त दस्तावेज सी) वेतनभोगी व्यक्ति: 1 है। रोजगार या प्रस्ताव पत्र 2. अंतिम वेतन पर्चियां <45 दिन 3. पिछले छह महीने के बैंक स्टेटमेंट और नवीनतम फॉर्म 16 डी) स्व-नियोजित: 1 है। पिछले तीन वर्षों का आयकर रिटर्न 2. पिछले छह महीने के बैंक स्टेटमेंट 3. बिक्री कर रिटर्न 4. टीडीएस प्रमाण पत्र 5. उपर्युक्त दस्तावेजों के आधार पर, ऋणदाता आपके दुपहिया ऋण आवेदन को प्रोसेस करेगा और आप अपने ऋण अनुमोदन पर सकारात्मक परिणाम की उम्मीद कर सकते हैं।\"\n",
        "}"
      ],
      "metadata": {
        "id": "eb3TixOjpjYx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRlrd2zJlioQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "indicTrans_python_interface.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}