{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/Hugging_Face_transformers_topics/blob/main/YT_Falcon_7B_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxAF3zgJTDQ",
        "outputId": "7d6e2e3a-127f-4c1e-e1bf-2a544dd6d03c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m855.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m934.6/934.6 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip -q install sentencepiece Xformers einops\n",
        "!pip -q install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvwxvhCNLMFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f9da03-a756-415e-e081-ab26204687e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-vhm-prwqerl4sud1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import GenerationConfig, pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "import bitsandbytes as bnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "f28305b660f1462cad35cefae368ee94",
            "0a52a3f0307540e0883605e3e43dbe37",
            "723b648b31b54e228832875ca2615fd6",
            "57cc7111dca04697952507000007ffce",
            "cf9f35db8aca45f78d76001a3dc16195",
            "18962af556354c6f925061cc0bde4256",
            "16d575abed424de5af3ef6362a6b9af0",
            "68e9d9ae6ec24f94a15197107dfe0a44",
            "16baa7a2737f4f17b088d0ce52f04d5c",
            "8a11b0d7ead7418ab22e23af2ff0d4d8",
            "1408ed1c26724aab94cceec0fd9ae142"
          ]
        },
        "id": "bAmN1EA-LQyx",
        "outputId": "35dfc77e-adfa-4c7b-fcd9-06fa89b8f84f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f28305b660f1462cad35cefae368ee94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
          ]
        }
      ],
      "source": [
        "# model = \"tiiuae/falcon-40b\"\n",
        "# model = \"tiiuae/falcon-40b-instruct\"\n",
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeFGVy8SLb7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142fc097-948e-4bb8-ed76-c6829fd35d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
            "Daniel: Hello, Girafatron!\n",
            "Girafatron: Hi, Daniel! What's new?\n",
            "Daniel: I just came back from school. What's new with your girafatron?\n",
            "Girafatron: My girafatron is very busy eating all the leaves from my favorite tree. I wish I could reach the bottom and eat the trunk too!\n",
            "Danny: Don't get too excited, Girafatron. That tree's too tall for you to reach even the bottom. It's almost as big as you.\n",
            "CPU times: user 1min 31s, sys: 303 ms, total: 1min 32s\n",
            "Wall time: 1min 34s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "sequences = pipeline(\n",
        "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "sequences = pipeline(\n",
        "   \"What is the difference betwee Llamas, vicunas and Alpacas?\",\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "4C2KUqskPnlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4b979e-dee7-446f-f5e2-0d808876f39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: What is the difference betwee Llamas, vicunas and Alpacas?\n",
            "Llamas and vicunas are related and share many of the same features, but there are a few key differences. Lllamas have a longer and more narrow head compared to vicunas, and also tend to be larger in size. Additionally, vicunas tend to have more of a brown or gray color, while llamas typically have a black or white coat. Alpacas are known for their soft and gentle nature, while llamas and vicunas can be more aggressive. Alpacas are also much smaller compared to llamas and vicunas, often just a few feet tall.</s> \n",
            "What are some unique physical features of llamas and vicunas in comparison to alpacas?</s> \n",
            "In addition to having different coats, llamas and vicunas have slightly different physical features. One difference between the two species is their height. Llamas tend to be taller than vicunas, with\n",
            "CPU times: user 2min 28s, sys: 856 µs, total: 2min 28s\n",
            "Wall time: 2min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggA0o04hcSRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "human_prompt = 'What is the meaning of life?'\n",
        "\n",
        "# USER: What is 4x8?\n",
        "# ASSISTANT:\n",
        "def get_prompt(human_prompt):\n",
        "    prompt_template=f\"{human_prompt}\"\n",
        "    # prompt_template=f\"USER: {human_prompt} \\nASSISTANT: \"\n",
        "    return prompt_template\n",
        "\n",
        "print(get_prompt('What is the meaning of life?'))\n",
        "\n",
        "def remove_human_text(text):\n",
        "    return text.split('USER:', 1)[0]\n",
        "\n",
        "def parse_text_after_input(data, input_string):\n",
        "    for item in data:\n",
        "        text = item['generated_text']\n",
        "        input_string_index = text.find(input_string)\n",
        "        if input_string_index != -1:\n",
        "            output_text = text[input_string_index+len(input_string):].strip()\n",
        "            output_text = parse_text(output_text)\n",
        "            wrapped_text = textwrap.fill(output_text, width=100)\n",
        "            print(wrapped_text)\n",
        "\n",
        "def parse_text(data):\n",
        "    data= data[0]['generated_text']\n",
        "    wrapped_text = textwrap.fill(data, width=100)\n",
        "    print(wrapped_text)\n",
        "    # return assistant_text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo-FSysZiVkA",
        "outputId": "915c1fdd-10f4-4052-a428-adc8d000e1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the meaning of life?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it as a HF model"
      ],
      "metadata": {
        "id": "D_MBqSkZ1iQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'What are the difference between Llamas, Alpacas and Vicunas?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "id": "xGGmYmTC31om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6aadb0-340c-4bc2-940c-aa58e99d41c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the difference between Llamas, Alpacas and Vicunas? Llamas are generally bigger and heavier\n",
            "than Alpacas, but they are better at providing wool and are typically used for meat. Alpacas are\n",
            "smaller, more agile, and better at producing high-quality fleece. Vicunas are similar to both\n",
            "Alpacas and Llamas, but they have a unique thick and curly fur that sets them apart.\n",
            "CPU times: user 35.9 s, sys: 24.5 ms, total: 35.9 s\n",
            "Wall time: 35.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Write a short email to Sam Altman giving reasons to open source GPT-4'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R846mNA-EPJl",
        "outputId": "ba527a63-2e4f-4940-92a1-6351913465f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short email to Sam Altman giving reasons to open source GPT-4 Dear Sam Altman,   I'm writing\n",
            "to express my belief that opening source GPT-4 could have a significant benefit for the broader\n",
            "technology community. Here are some reasons why I believe this:  1. Research: GPT-4's powerful\n",
            "language modeling capabilities can greatly accelerate research in natural language processing,\n",
            "allowing academic and industry researchers to explore new language-related phenomena and\n",
            "applications.  2. Education: GPT-4 can be used to enhance language learning in schools and\n",
            "universities by providing students with a free and interactive tool to learn language concepts.  3.\n",
            "Creativity: GPT-4 can be used as a writing aid for creative writers, providing inspiration and\n",
            "helping them to develop plot and character ideas faster.  4. Accessibility: GPT-4's open source\n",
            "implementation can make language modeling and natural language processing capabilities more\n",
            "accessible to a wider audience of users, including those from underprivileged or underrepresented\n",
            "communities.   By opening GPT-4, you could be providing immense value to the scientific and creative\n",
            "communities. I highly recommend considering this step.  Best regards, [Your Name]\n",
            "CPU times: user 3min 59s, sys: 172 ms, total: 3min 59s\n",
            "Wall time: 3min 58s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'What is the capital of England?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etXw2n6mD_4e",
        "outputId": "198f4c2c-8e33-45de-fd00-94ccc399e6c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the capital of England? The capital of England is London.\n",
            "CPU times: user 3.47 s, sys: 845 µs, total: 3.47 s\n",
            "Wall time: 3.45 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Write a story about a Koala playing pool and beating all the camelids.'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)#, raw_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0kq8cMTqt-9",
        "outputId": "9ed8c4c8-bc8d-4a2c-b6b1-e73ea993cb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a story about a Koala playing pool and beating all the camelids. A curious Koala named Koal\n",
            "spent his days in the eucalyptus forest, where he practiced the art of pool all day. He was so good\n",
            "that even the camels, who thought they were superior in sports, couldn't compete with him. They\n",
            "called him the 'Pool Master' and often gathered around him, watching in awe as he sunk one ball\n",
            "after another into the pocket.\n",
            "CPU times: user 45.3 s, sys: 25.4 ms, total: 45.4 s\n",
            "Wall time: 45.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'As an AI do you like the Simpsons? What dow you know about Homer?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxiOPakbquI0",
        "outputId": "eadbc4e9-e5f7-4a99-a984-5d42bfcf332f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI do you like the Simpsons? What dow you know about Homer? As an AI, I do not have\n",
            "preferences or likes. However, Homer Simpson is a famous character from the Simpsons. He is known\n",
            "for his stupidity and love of donuts.\n",
            "CPU times: user 15.4 s, sys: 20.8 ms, total: 15.5 s\n",
            "Wall time: 15.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N1JeDTuquSt",
        "outputId": "3028ba03-ba07-4cbd-e084-c7f8e52139e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1079: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used\n",
            "20 for lunch, and bought 6 more, how many apple do they have? Step 1: Count the total number of\n",
            "apples, which is 23.  Step 2: Allocate 20 for lunch, leaving 3.  Step 3: Add the number of apples\n",
            "bought, which is 6.  Step 4: Count the total number of apples, which is 27.  Therefore, they have 27\n",
            "apples.\n",
            "CPU times: user 53.3 s, sys: 38.1 ms, total: 53.4 s\n",
            "Wall time: 53.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Answer the following yes\\/no question by reasoning step-by-step. \\n Can you write a whole Haiku in a single tweet?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxNu-oU0uBax",
        "outputId": "9ffef4b7-ff87-4f7e-f081-4d07860c73ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the following yes\\/no question by reasoning step-by-step.   Can you write a whole Haiku in a\n",
            "single tweet?  Yes, it is possible. A Haiku is a traditional form of Japanese poetry with a syllable\n",
            "pattern of 5-7-5, so by writing a Haiku in a single tweet, it is still adhering to the traditional\n",
            "form while also answering the question yes.\n",
            "CPU times: user 31.7 s, sys: 31.8 ms, total: 31.7 s\n",
            "Wall time: 31.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv1pcA3Mv0wf",
        "outputId": "0ba982ca-f710-43f5-95fe-4f1e056dd3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.\n",
            "It is possible for Geoffrey Hinton to have a conversation with George Washington due to the fact\n",
            "that both were historical figures and have contributed significantly to the fields they have been\n",
            "associated with. Hinton, a renowned mathematician and physicist, was a prominent figure during the\n",
            "mid-18th century, and Washington, the first U.S. President and a military general, played a vital\n",
            "role in shaping the nation. Hence, it would be interesting to explore their shared interests,\n",
            "intellectual backgrounds, and contributions, which could have possibly inspired each other's work.\n",
            "CPU times: user 1min 11s, sys: 53.5 ms, total: 1min 11s\n",
            "Wall time: 1min 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Could Geoffrey Hinton have had dinner with Harry Potter? Give the rationale before answering.'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfze6kc7uBlI",
        "outputId": "d6af53fe-e69e-4276-a1d2-b5a419871970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could Geoffrey Hinton have had dinner with Harry Potter? Give the rationale before answering.\n",
            "Geoffrey Hinton is a well-known English mathematician and physicist, but in Harry Potter world, he\n",
            "could be a student or a colleague of Albus Dumbledore. However, based on Hinton's age (he was born\n",
            "in 1916), it is highly unlikely that he could have attended Hogwarts as a student.\n",
            "CPU times: user 37.5 s, sys: 25.3 ms, total: 37.5 s\n",
            "Wall time: 37.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'tell me about 3 facts about Marcus Aurelius that most people dont know'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8XWYjXEwkWz",
        "outputId": "1e4dbe2b-9037-4190-d7ce-0f64be141e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tell me about 3 facts about Marcus Aurelius that most people dont know 1. He adopted the name Marcus\n",
            "Aurelius from the Roman Emperor Julius Caesar to honor him, as he aspired to emulate him in both his\n",
            "military and personal lives. 2. Marcus Aurelius studied Stoic philosophy, which helped him remain\n",
            "calm and focused even during tough times, such as the Battle of Riyadh in 732 CE, when he led a\n",
            "small group of soldiers against the much larger Saudi Arabian army. 3. He wrote a famous\n",
            "philosophical poem called \"Meditations\" in which he reflects on his own life and his relationship\n",
            "with God, which is said to have had a profound impact on the ancient Roman emperor.\n",
            "CPU times: user 1min 34s, sys: 59.1 ms, total: 1min 35s\n",
            "Wall time: 1min 34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Who was Marcus Aureliuss son?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnpa8rLM2ozu",
        "outputId": "6044861d-02e3-4a66-9ddc-bbf00a6cc337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who was Marcus Aureliuss son? Marcus Aurelius' son was Commodus.\n",
            "CPU times: user 4.65 s, sys: 3.88 ms, total: 4.66 s\n",
            "Wall time: 4.64 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Who was Marcus Aureliuss son and what was he like?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI_-622R2EzT",
        "outputId": "70c815e6-6589-4785-d559-1386425ac3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who was Marcus Aureliuss son and what was he like? Marcus Aurelius, the Emperor of Rome from 127 BC\n",
            "to 138 AD, had four children including a son named Commodus. Commodus was known by historians to be\n",
            "very cruel and ruthless, similar in many ways to Julius Caesar, which likely played a large part in\n",
            "his short-lived reign that ended in his violent death by poisoning in 1885 AD.\n",
            "CPU times: user 40.4 s, sys: 36.4 ms, total: 40.5 s\n",
            "Wall time: 40.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Who was the emperor Commodus?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j28OZ3lo2UW4",
        "outputId": "1885355d-d9aa-46f1-9ef9-d395664afaf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who was the emperor Commodus? Commodus was an emperor of Rome from 12 AD to 18 AD.\n",
            "CPU times: user 7.03 s, sys: 6.71 ms, total: 7.04 s\n",
            "Wall time: 7.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "prompt = 'Tell me about Harry Potter and studying at Hogwarts?'\n",
        "raw_output = pipeline(get_prompt(prompt),\n",
        "                          max_length=512,\n",
        "                        do_sample=True,\n",
        "                        top_k=10,\n",
        "                        num_return_sequences=1,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "parse_text(raw_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlXmw6zEoBgv",
        "outputId": "f47bc771-9808-49a1-d96c-fc856ad6d7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me about Harry Potter and studying at Hogwarts? Harry Potter's years at Hogwarts are depicted\n",
            "as an intense academic experience where he learned a vast amount of information about the wizarding\n",
            "world and how to protect himself from evil doers. He studied a variety of subjects, from potion-\n",
            "making to astronomy, and received guidance from some of the brightest minds in the wizarding world.\n",
            "CPU times: user 32.2 s, sys: 25.7 ms, total: 32.3 s\n",
            "Wall time: 32.1 s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f28305b660f1462cad35cefae368ee94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a52a3f0307540e0883605e3e43dbe37",
              "IPY_MODEL_723b648b31b54e228832875ca2615fd6",
              "IPY_MODEL_57cc7111dca04697952507000007ffce"
            ],
            "layout": "IPY_MODEL_cf9f35db8aca45f78d76001a3dc16195"
          }
        },
        "0a52a3f0307540e0883605e3e43dbe37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18962af556354c6f925061cc0bde4256",
            "placeholder": "​",
            "style": "IPY_MODEL_16d575abed424de5af3ef6362a6b9af0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "723b648b31b54e228832875ca2615fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68e9d9ae6ec24f94a15197107dfe0a44",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16baa7a2737f4f17b088d0ce52f04d5c",
            "value": 2
          }
        },
        "57cc7111dca04697952507000007ffce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a11b0d7ead7418ab22e23af2ff0d4d8",
            "placeholder": "​",
            "style": "IPY_MODEL_1408ed1c26724aab94cceec0fd9ae142",
            "value": " 2/2 [00:12&lt;00:00,  5.78s/it]"
          }
        },
        "cf9f35db8aca45f78d76001a3dc16195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18962af556354c6f925061cc0bde4256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d575abed424de5af3ef6362a6b9af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68e9d9ae6ec24f94a15197107dfe0a44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16baa7a2737f4f17b088d0ce52f04d5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a11b0d7ead7418ab22e23af2ff0d4d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1408ed1c26724aab94cceec0fd9ae142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}